{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PytorchTut.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q-nLVTY_LwXh"},"source":["# Pytorch Basics\n","\n","\n","#### Table of contents\n","0. Pytorch tensors\n","\n","1. Basic autograd example 1\n","\n","2. Basic autograd example 2\n","\n","3. Loading data from numpy \n","\n","4. Input pipeline           \n","\n","5. Input pipeline for custom dataset \n","\n","6. Pretrained model                 \n","\n","7. Save and load model\n","\n","8. Train a simple MNIST Neural nets\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"LeO-UXC-OZ_A"},"source":["import torch \n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bwEVQF5xagPM"},"source":["##0. Pytorch tensors"]},{"cell_type":"code","metadata":{"id":"e4YZOt2tajAB"},"source":["# Very similar to numpy\n","x = torch.tensor(1.)\n","print(x)\n","\n","x = torch.tensor([1,2,3,4,5]).float()\n","print(x)\n","print(x.size())\n","\n","ran = torch.Tensor(2,5,5).uniform_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQjMkMYia60M"},"source":["# min, max, mean, reshaping\n","ran = torch.Tensor(2,5,5).uniform_()\n","print(ran)\n","print(ran.size())\n","print(ran.min())\n","print(ran.max())\n","print(ran.mean())\n","rsan = ran.view(5,2,5)\n","print(rsan)\n","print(rsan.size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZthx9bibTog"},"source":["# Tensor math\n","x = torch.Tensor(2, 4, 5).uniform_()\n","y = torch.Tensor(2, 4, 5).uniform_()\n","print(x.size())\n","print(y.size())\n","print(x)\n","print(y)\n","a = x + y\n","m = x * y\n","# transpose of y (the last 2 dimension)\n","yt = y.transpose(1, 2)\n","print(yt.size())\n","matmul = torch.matmul(x, yt)\n","print(matmul.size())\n","print(matmul)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NvmBBRDnPjIH"},"source":["##1. Basic autograd example 1"]},{"cell_type":"code","metadata":{"id":"5iHZVz5GOdbb"},"source":["# Create tensors.\n","x = torch.tensor(1., requires_grad=True)\n","w = torch.tensor(2., requires_grad=True)\n","b = torch.tensor(3., requires_grad=True)\n","\n","# Build a computational graph.\n","y = w * x + b    # y = 2 * x + 3\n","\n","# Compute gradients.\n","y.backward()\n","\n","# Print out the gradients.\n","print(x.grad)    # x.grad = 2 \n","print(w.grad)    # w.grad = 1 \n","print(b.grad)    # b.grad = 1 \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nMe0u_oPyD-"},"source":["##1. Basic autograd example 1"]},{"cell_type":"code","metadata":{"id":"oUvKhddOPcBM","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1573203612612,"user_tz":-480,"elapsed":2969,"user":{"displayName":"Xuan Phi Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBMe29t94knDbWFUTdAsYnOA_RwiofW5MdraN0v1TU=s64","userId":"15291677822697214066"}},"outputId":"4e684706-ef6c-453d-e5af-cb895b7a4856"},"source":["# Create tensors of shape (10, 3) and (10, 2).\n","x = torch.randn(10, 3)\n","y = torch.randn(10, 2)\n","\n","# Build a fully connected layer.\n","linear = nn.Linear(3, 2)\n","print ('w: ', linear.weight)\n","print ('b: ', linear.bias)\n","\n","# Build loss function and optimizer.\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n","\n","# Forward pass.\n","pred = linear(x)\n","\n","# Compute loss.\n","loss = criterion(pred, y)\n","print('loss: ', loss.item())\n","\n","# Backward pass.\n","loss.backward()\n","\n","# Print out the gradients.\n","print ('dL/dw: ', linear.weight.grad) \n","print ('dL/db: ', linear.bias.grad)\n","\n","# 1-step gradient descent.\n","optimizer.step()\n","\n","# You can also perform gradient descent at the low level.\n","# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n","# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n","\n","# Print out the loss after 1-step gradient descent.\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('loss after 1 step optimization: ', loss.item())\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w:  Parameter containing:\n","tensor([[-0.0474,  0.0154,  0.1638],\n","        [-0.1966, -0.2304,  0.4822]], requires_grad=True)\n","b:  Parameter containing:\n","tensor([-0.2659,  0.1617], requires_grad=True)\n","loss:  0.8229060769081116\n","dL/dw:  tensor([[ 0.3962,  0.3727,  0.3502],\n","        [ 0.1079, -0.2590,  0.1530]])\n","dL/db:  tensor([-0.5923, -0.2813])\n","loss after 1 step optimization:  0.8134800791740417\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KiJypsSQPzUC"},"source":["##3. Loading data from numpy"]},{"cell_type":"code","metadata":{"id":"RpFqOS4GQMeC"},"source":["# Create a numpy array.\n","x = np.array([[1, 2], [3, 4]])\n","\n","# Convert the numpy array to a torch tensor.\n","y = torch.from_numpy(x)\n","\n","# Convert the torch tensor to a numpy array.\n","z = y.numpy()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8BGz1VxP2JY"},"source":["##4. Input pipeline"]},{"cell_type":"code","metadata":{"id":"hqXhTlCxQPwy"},"source":["# Download and construct CIFAR-10 dataset.\n","train_dataset = torchvision.datasets.CIFAR10(\n","  root='.',\n","  train=True, \n","  transform=transforms.ToTensor(),\n","  download=True)\n","\n","# Fetch one data pair (read data from disk).\n","image, label = train_dataset[0]\n","print (image.size())\n","print (label)\n","\n","# Data loader (this provides queues and threads in a very simple way).\n","train_loader = torch.utils.data.DataLoader(\n","    dataset=train_dataset,\n","    batch_size=64, \n","    shuffle=True)\n","\n","# When iteration starts, queue and thread start to load data from files.\n","data_iter = iter(train_loader)\n","\n","# Mini-batch images and labels.\n","images, labels = data_iter.next()\n","\n","# Actual usage of the data loader is as below.\n","for images, labels in train_loader:\n","    # Training code should be written here.\n","    pass\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"22zI4Mi-P3nk"},"source":["##5. Input pipeline for custom dataset"]},{"cell_type":"code","metadata":{"id":"km9Ma1TTQcZB"},"source":["# You should build your custom dataset as below.\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self):\n","        # TODO\n","        # 1. Initialize file paths or a list of file names. \n","        pass\n","    def __getitem__(self, index):\n","        # TODO\n","        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n","        # 2. Preprocess the data (e.g. torchvision.Transform).\n","        # 3. Return a data pair (e.g. image and label).\n","        pass\n","    def __len__(self):\n","        # You should change 0 to the total size of your dataset.\n","        return 0 \n","\n","# You can then use the prebuilt data loader. \n","custom_dataset = CustomDataset()\n","train_loader = torch.utils.data.DataLoader(\n","    dataset=custom_dataset,\n","    batch_size=64, \n","    shuffle=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyDUflRGP3v_"},"source":["##6. Pretrained model"]},{"cell_type":"code","metadata":{"id":"ez0wfFfTQg06"},"source":["# Download and load the pretrained ResNet-18.\n","resnet = torchvision.models.resnet18(pretrained=True)\n","\n","# If you want to finetune only the top layer of the model, set as below.\n","for param in resnet.parameters():\n","    param.requires_grad = False\n","\n","# Replace the top layer for finetuning.\n","resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n","\n","# Forward pass.\n","images = torch.randn(64, 3, 224, 224)\n","outputs = resnet(images)\n","print (outputs.size())     # (64, 100)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oX-SQZO0P36F"},"source":["##7. Save and load model"]},{"cell_type":"code","metadata":{"id":"zDpSgZKFQjKJ"},"source":["# Save and load the entire model.\n","torch.save(resnet, 'model.ckpt')\n","model = torch.load('model.ckpt')\n","\n","# Save and load only the model parameters (recommended).\n","torch.save(resnet.state_dict(), 'params.ckpt')\n","resnet.load_state_dict(torch.load('params.ckpt'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iK1YQYVgUurI"},"source":["##8. Train a simple MNIST Neural nets\n","Let's train a simple neural network to classify MNIST hand-written digit\n"]},{"cell_type":"markdown","metadata":{"id":"8WyC0tZ2U-rX"},"source":["####1. Download the dataset"]},{"cell_type":"code","metadata":{"id":"1TRyf_4lUwqC"},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","input_size = 784\n","hidden_size = 500\n","num_classes = 10\n","num_epochs = 5\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# MNIST dataset \n","train_dataset = torchvision.datasets.MNIST(root='.', \n","                                           train=True, \n","                                           transform=transforms.ToTensor(),  \n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='.', \n","                                          train=False, \n","                                          transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BEDqrM6VGNd","colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"status":"ok","timestamp":1573205236173,"user_tz":-480,"elapsed":3175,"user":{"displayName":"Xuan Phi Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBMe29t94knDbWFUTdAsYnOA_RwiofW5MdraN0v1TU=s64","userId":"15291677822697214066"}},"outputId":"79f68291-d9f5-4836-91c2-cd49e455e530"},"source":["image, label = test_dataset[0]\n","# reduce batch=1 to no batch\n","image = image[0]\n","print(f'{image.size()} , Label: {label}')\n","plt.imshow(image)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([28, 28]) , Label: 7\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fb19265f780>"]},"metadata":{"tags":[]},"execution_count":13},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngrADSRB2IAnCDiRB2IEk/h9BCfQTVPflJQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"CDYcpmKeVx3S"},"source":["####2. Initiate the Neural Network (multi-layer perceptron)\n","The network has 2 layers, with ReLu activation in between\n","\n","![](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n"]},{"cell_type":"code","metadata":{"id":"LGCTJ2CBVVmL"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","input_size = 784\n","hidden_size = 500\n","num_classes = 10\n","num_epochs = 5\n","batch_size = 100\n","learning_rate = 0.001\n","\n","\n","# Fully connected neural network with one hidden layer\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)  \n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n","\n","model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ihi_EnNxWETI"},"source":["####3. Train the network"]},{"cell_type":"code","metadata":{"id":"QLTt3Un5WCpo","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1573205407448,"user_tz":-480,"elapsed":49262,"user":{"displayName":"Xuan Phi Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBMe29t94knDbWFUTdAsYnOA_RwiofW5MdraN0v1TU=s64","userId":"15291677822697214066"}},"outputId":"5c8b26bd-d9ff-4878-b4f6-a5cc5408e2eb"},"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # Move tensors to the configured device\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/5], Step [100/600], Loss: 0.2173\n","Epoch [1/5], Step [200/600], Loss: 0.3335\n","Epoch [1/5], Step [300/600], Loss: 0.1924\n","Epoch [1/5], Step [400/600], Loss: 0.1521\n","Epoch [1/5], Step [500/600], Loss: 0.2531\n","Epoch [1/5], Step [600/600], Loss: 0.1566\n","Epoch [2/5], Step [100/600], Loss: 0.1063\n","Epoch [2/5], Step [200/600], Loss: 0.1457\n","Epoch [2/5], Step [300/600], Loss: 0.1089\n","Epoch [2/5], Step [400/600], Loss: 0.0812\n","Epoch [2/5], Step [500/600], Loss: 0.2208\n","Epoch [2/5], Step [600/600], Loss: 0.1829\n","Epoch [3/5], Step [100/600], Loss: 0.0711\n","Epoch [3/5], Step [200/600], Loss: 0.0966\n","Epoch [3/5], Step [300/600], Loss: 0.1316\n","Epoch [3/5], Step [400/600], Loss: 0.1719\n","Epoch [3/5], Step [500/600], Loss: 0.0641\n","Epoch [3/5], Step [600/600], Loss: 0.0860\n","Epoch [4/5], Step [100/600], Loss: 0.0132\n","Epoch [4/5], Step [200/600], Loss: 0.0497\n","Epoch [4/5], Step [300/600], Loss: 0.0247\n","Epoch [4/5], Step [400/600], Loss: 0.1300\n","Epoch [4/5], Step [500/600], Loss: 0.0533\n","Epoch [4/5], Step [600/600], Loss: 0.0640\n","Epoch [5/5], Step [100/600], Loss: 0.0239\n","Epoch [5/5], Step [200/600], Loss: 0.0781\n","Epoch [5/5], Step [300/600], Loss: 0.0574\n","Epoch [5/5], Step [400/600], Loss: 0.0903\n","Epoch [5/5], Step [500/600], Loss: 0.0463\n","Epoch [5/5], Step [600/600], Loss: 0.0289\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SlOaxGskWMFo"},"source":["####4. Test the network"]},{"cell_type":"code","metadata":{"id":"6VKlDUX8WLNj","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1573205417197,"user_tz":-480,"elapsed":2726,"user":{"displayName":"Xuan Phi Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBMe29t94knDbWFUTdAsYnOA_RwiofW5MdraN0v1TU=s64","userId":"15291677822697214066"}},"outputId":"96cb3f78-cf73-4c1b-c572-9a415cd28746"},"source":["# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.pt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 97.96 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lEidschCWZH3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mFcrwxgYW9BE"},"source":["#### Credits: https://github.com/keineahnung2345/pytorch-tutorial-jupyter-notebooks"]}]}